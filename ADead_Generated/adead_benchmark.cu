// ADead-BIB + CUDA - Full Benchmark Suite v2.0
// Instrumentación CORRECTA con cudaEvent (no clock())
// Métricas separadas: H2D, Kernel, D2H
// Tiempos en microsegundos para precisión

#include <stdio.h>
#include <stdlib.h>
#include <cuda_runtime.h>
#include <chrono>

using namespace std::chrono;

// ============================================
// KERNELS GPU
// ============================================

__global__ void vectorAdd(float *A, float *B, float *C, int n) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < n) {
        C[i] = A[i] + B[i];
    }
}

__global__ void vectorMul(float *A, float *B, float *C, int n) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < n) {
        C[i] = A[i] * B[i];
    }
}

__global__ void saxpy(float a, float *x, float *y, int n) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

// ============================================
// CPU VERSIONS
// ============================================

void cpu_vectorAdd(float *A, float *B, float *C, int n) {
    for (int i = 0; i < n; i++) C[i] = A[i] + B[i];
}

void cpu_vectorMul(float *A, float *B, float *C, int n) {
    for (int i = 0; i < n; i++) C[i] = A[i] * B[i];
}

void cpu_saxpy(float a, float *x, float *y, int n) {
    for (int i = 0; i < n; i++) y[i] = a * x[i] + y[i];
}

// ============================================
// BENCHMARK STRUCTURE
// ============================================

struct BenchResult {
    double cpu_us;      // CPU time in microseconds
    double h2d_us;      // Host to Device transfer
    double kernel_us;   // Kernel execution only
    double d2h_us;      // Device to Host transfer
    double gpu_total_us; // Total GPU time (H2D + kernel + D2H)
    long long flops;    // Floating point operations
};

void print_header() {
    printf("\n");
    printf("╔══════════════════════════════════════════════════════════════════════════════╗\n");
    printf("║  ADead-BIB + CUDA Benchmark Suite v2.0                                       ║\n");
    printf("║  Instrumentación: cudaEvent (precisión microsegundos)                        ║\n");
    printf("║  Métricas: H2D, Kernel, D2H separadas                                        ║\n");
    printf("╚══════════════════════════════════════════════════════════════════════════════╝\n");
    printf("\n");
}

void print_result(const char* name, int n, BenchResult r) {
    double speedup_kernel = r.cpu_us / (r.kernel_us > 0.1 ? r.kernel_us : 0.1);
    double speedup_total = r.cpu_us / (r.gpu_total_us > 0.1 ? r.gpu_total_us : 0.1);
    double gflops = (r.flops / 1e9) / (r.kernel_us / 1e6);
    double bandwidth_gb = (n * sizeof(float) * 3.0 / 1e9) / (r.kernel_us / 1e6);
    
    printf("%-12s | n=%8d\n", name, n);
    printf("  CPU:        %10.1f µs\n", r.cpu_us);
    printf("  GPU H2D:    %10.1f µs\n", r.h2d_us);
    printf("  GPU Kernel: %10.1f µs\n", r.kernel_us);
    printf("  GPU D2H:    %10.1f µs\n", r.d2h_us);
    printf("  GPU Total:  %10.1f µs\n", r.gpu_total_us);
    printf("  Speedup (kernel-only): %6.1fx\n", speedup_kernel);
    printf("  Speedup (end-to-end):  %6.1fx\n", speedup_total);
    printf("  GFLOPS:     %10.2f\n", gflops);
    printf("  Bandwidth:  %10.2f GB/s\n", bandwidth_gb);
    printf("\n");
}

// ============================================
// MAIN BENCHMARK
// ============================================

int main() {
    print_header();
    
    // Warmup GPU
    float *d_warmup;
    cudaMalloc(&d_warmup, 1024);
    cudaFree(d_warmup);
    cudaDeviceSynchronize();
    
    int sizes[] = {10000, 100000, 1000000, 10000000};
    int num_sizes = 4;
    
    for (int s = 0; s < num_sizes; s++) {
        int n = sizes[s];
        size_t size = n * sizeof(float);
        
        printf("════════════════════════════════════════════════════════════════════════════════\n");
        printf("Size: %d elements (%.2f MB)\n", n, size / (1024.0 * 1024.0));
        printf("════════════════════════════════════════════════════════════════════════════════\n\n");
        
        // Allocate host memory
        float *h_A = (float*)malloc(size);
        float *h_B = (float*)malloc(size);
        float *h_C = (float*)malloc(size);
        
        // Initialize
        for (int i = 0; i < n; i++) {
            h_A[i] = (float)(i % 100) / 100.0f;
            h_B[i] = (float)((i + 50) % 100) / 100.0f;
        }
        
        // Allocate device memory
        float *d_A, *d_B, *d_C;
        cudaMalloc(&d_A, size);
        cudaMalloc(&d_B, size);
        cudaMalloc(&d_C, size);
        
        // CUDA events for precise timing
        cudaEvent_t start, stop;
        cudaEventCreate(&start);
        cudaEventCreate(&stop);
        
        int threadsPerBlock = 256;
        int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;
        
        BenchResult result;
        float elapsed_ms;
        
        // ============================================
        // TEST: VectorAdd
        // ============================================
        
        // CPU timing with high_resolution_clock
        auto cpu_start = high_resolution_clock::now();
        cpu_vectorAdd(h_A, h_B, h_C, n);
        auto cpu_end = high_resolution_clock::now();
        result.cpu_us = duration_cast<microseconds>(cpu_end - cpu_start).count();
        result.flops = (long long)n;  // 1 FLOP per element
        
        // GPU H2D
        cudaEventRecord(start);
        cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
        cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
        cudaEventRecord(stop);
        cudaEventSynchronize(stop);
        cudaEventElapsedTime(&elapsed_ms, start, stop);
        result.h2d_us = elapsed_ms * 1000.0;
        
        // GPU Kernel
        cudaEventRecord(start);
        vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n);
        cudaEventRecord(stop);
        cudaEventSynchronize(stop);
        cudaEventElapsedTime(&elapsed_ms, start, stop);
        result.kernel_us = elapsed_ms * 1000.0;
        
        // GPU D2H
        cudaEventRecord(start);
        cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
        cudaEventRecord(stop);
        cudaEventSynchronize(stop);
        cudaEventElapsedTime(&elapsed_ms, start, stop);
        result.d2h_us = elapsed_ms * 1000.0;
        
        result.gpu_total_us = result.h2d_us + result.kernel_us + result.d2h_us;
        print_result("VectorAdd", n, result);
        
        // ============================================
        // TEST: VectorMul
        // ============================================
        
        cpu_start = high_resolution_clock::now();
        cpu_vectorMul(h_A, h_B, h_C, n);
        cpu_end = high_resolution_clock::now();
        result.cpu_us = duration_cast<microseconds>(cpu_end - cpu_start).count();
        result.flops = (long long)n;
        
        // Data already on device, just kernel
        cudaEventRecord(start);
        vectorMul<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n);
        cudaEventRecord(stop);
        cudaEventSynchronize(stop);
        cudaEventElapsedTime(&elapsed_ms, start, stop);
        result.kernel_us = elapsed_ms * 1000.0;
        result.h2d_us = 0; // Already transferred
        result.d2h_us = 0; // Not transferring back
        result.gpu_total_us = result.kernel_us;
        print_result("VectorMul", n, result);
        
        // ============================================
        // TEST: SAXPY (y = a*x + y)
        // ============================================
        
        float alpha = 2.5f;
        
        cpu_start = high_resolution_clock::now();
        cpu_saxpy(alpha, h_A, h_B, n);
        cpu_end = high_resolution_clock::now();
        result.cpu_us = duration_cast<microseconds>(cpu_end - cpu_start).count();
        result.flops = (long long)n * 2;  // 2 FLOPs per element (mul + add)
        
        cudaEventRecord(start);
        saxpy<<<blocksPerGrid, threadsPerBlock>>>(alpha, d_A, d_B, n);
        cudaEventRecord(stop);
        cudaEventSynchronize(stop);
        cudaEventElapsedTime(&elapsed_ms, start, stop);
        result.kernel_us = elapsed_ms * 1000.0;
        result.h2d_us = 0;
        result.d2h_us = 0;
        result.gpu_total_us = result.kernel_us;
        print_result("SAXPY", n, result);
        
        // Cleanup
        cudaEventDestroy(start);
        cudaEventDestroy(stop);
        cudaFree(d_A);
        cudaFree(d_B);
        cudaFree(d_C);
        free(h_A);
        free(h_B);
        free(h_C);
    }
    
    printf("════════════════════════════════════════════════════════════════════════════════\n");
    printf("  NOTAS IMPORTANTES:\n");
    printf("  - Tiempos en microsegundos (µs) para precisión\n");
    printf("  - Speedup kernel-only: compara solo ejecución\n");
    printf("  - Speedup end-to-end: incluye transferencias (más realista)\n");
    printf("  - GFLOPS calculados sobre tiempo de kernel\n");
    printf("  - Bandwidth: bytes transferidos / tiempo kernel\n");
    printf("════════════════════════════════════════════════════════════════════════════════\n");
    printf("\n");
    printf("  ADead-BIB + CUDA Benchmark v2.0 Complete!\n");
    printf("  Generado por ADead-BIB v1.2.0\n");
    printf("\n");
    
    return 0;
}
